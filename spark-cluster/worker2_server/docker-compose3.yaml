#Worker 2 Node docker-compose.yaml
version: '3.8'
services:
  cadvisor:
    image: google/cadvisor:canary
    container_name: cadvisor-spark-worker-1
    ports:
      - "5500:8080"  # 호스트 포트를 5500으로 매핑
    volumes:
      - "/:/rootfs:ro"
      - "/var/run:/var/run:ro"
      - "/sys:/sys:ro"
      - "/var/lib/docker/:/var/lib/docker:ro"
      - "/dev/disk/:/dev/disk:ro" #디스크 I/O 관련 메트릭
    networks:
      - kafka_network
    restart: always

  spark-worker-2:
    build: 
      context: .  # 현재 디렉터리에서 빌드
      dockerfile: Dockerfile  # 'Dockerfile' 파일을 사용하여 빌드
    container_name: spark-worker-2
    user: "root"
    hostname: spark-worker-2
    networks:
      kafka_network:
        
    environment:
      - SPARK_MODE=worker
      - SPARK_MASTER_HOST=spark-master:7077
      - SPARK_MASTER_URL=spark://10.178.0.26:7077
      - SPARK_WORKER_WEBUI_PORT=8082
      - SPARK_WORKER_MEMORY=2G
      - SPARK_WORKER_CORES=2
      - SPARK_WORKER_INSTANCES=1
      - SPARK_RPC_AUTHENTICATION_ENABLED=no
      - SPARK_RPC_ENCRYPTION_ENABLED=no
      - SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED=no
      - SPARK_SSL_ENABLED=no
      - SPARK_WORKER_OPTS=
        -Dspark.worker.cleanup.enabled=true
        -Dspark.worker.cleanup.interval=30
        -Dspark.worker.cleanup.appDataTtl=30
    ports:
      - "8082:8082"
    volumes:
      - ./data:/opt/spark/data
  
networks:
  kafka_network:
    external: true
