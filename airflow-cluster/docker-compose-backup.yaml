### GCS - Spark - Feast Cluster pipeline with Airflow ###
version: '3'

services:
  airflow-webserver:
    build: .
    user: "${AIRFLOW_UID:-50000}:0"
    environment:
      - AIRFLOW__CORE__EXECUTOR=SequentialExecutor
      - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=sqlite:////opt/airflow/airflow/airflow.db
      - AIRFLOW__CORE__LOAD_EXAMPLES=false
      - AIRFLOW__CORE__FERNET_KEY=${FERNET_KEY}
      - AIRFLOW__WEBSERVER__SECRET_KEY=${WEBSERVER_SECRET_KEY}
      # Spark & GCS 설정
      - AIRFLOW__CORE__ENABLE_XCOM_PICKLING=true
      - SPARK_MASTER_URL=spark://${SPARK_MASTER_IP}:7077
      - GCP_BUCKET_NAME=${GCP_BUCKET_NAME}
      - GCP_PROJECT_ID=${GCP_PROJECT_ID}
      - GOOGLE_APPLICATION_CREDENTIALS=/opt/airflow/config/gcp-credentials.json
      # Feast 설정
      - FEAST_REGISTRY_PATH=/opt/airflow/data/registry.db
    volumes:
      - ./dags:/opt/airflow/dags
      - ./logs:/opt/airflow/logs
      - ./plugins:/opt/airflow/plugins
      - ./config:/opt/airflow/config
      - ./data:/opt/airflow/data
      - ./airflow:/opt/airflow/airflow
      - ./config/gcp-credentials.json:/opt/airflow/config/gcp-credentials.json:ro
      - ./jars:/opt/airflow/jars
    ports:
      - "8080:8080"
    command: bash -c "airflow db init && airflow db migrate && airflow users create --username admin --firstname admin --lastname admin --role Admin --email admin@example.com --password admin && airflow webserver"
    healthcheck:
      test: ["CMD", "curl", "--fail", "http://localhost:8080/health"]
      interval: 30s
      timeout: 10s
      retries: 5
    restart: always
    hostname: airflow-webserver
    networks:
      - airflow-spark-net

  airflow-scheduler:
    build: .
    user: "${AIRFLOW_UID:-50000}:0"
    environment:
      - AIRFLOW__CORE__EXECUTOR=SequentialExecutor
      - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=sqlite:////opt/airflow/airflow/airflow.db
      - AIRFLOW__CORE__LOAD_EXAMPLES=false
      - AIRFLOW__CORE__FERNET_KEY=${FERNET_KEY}
      - SPARK_MASTER_URL=spark://${SPARK_MASTER_IP}:7077
      - GCP_BUCKET_NAME=${GCP_BUCKET_NAME}
      - GCP_PROJECT_ID=${GCP_PROJECT_ID}
      - GOOGLE_APPLICATION_CREDENTIALS=/opt/airflow/config/gcp-credentials.json
      - FEAST_REGISTRY_PATH=/opt/airflow/data/registry.db
    volumes:
      - ./dags:/opt/airflow/dags
      - ./logs:/opt/airflow/logs
      - ./plugins:/opt/airflow/plugins
      - ./config:/opt/airflow/config
      - ./data:/opt/airflow/data
      - ./airflow:/opt/airflow/airflow
      - ./config/gcp-credentials.json:/opt/airflow/config/gcp-credentials.json:ro
      - ./jars:/opt/airflow/jars
    command: bash -c "airflow db upgrade && airflow scheduler"
    restart: always
    hostname: airflow-scheduler
    networks:
      - airflow-spark-net

networks:
  airflow-spark-net:
    driver: bridge
